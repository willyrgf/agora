# Causal Information Preserving Decentralized Network

## Problem 1
The Internet as a decentralized information network has scatered unorganized data around the world into little pockets behind data providers, paywalls or non standardized accessibility. When accessible, this data is usually unstructured and meaningless. By this definition, there's no uniform, unified way to transform this unorganized, unstructured data into a organized, structured, meaningful information unifiedly accessible for people, organizations and programs.

## Conjecture 1
An open distributed network composed by nodes, which each node can: (1) as a producer, freely upload information as scheme (structure/ontology) and data (data that conforms into the structure/ontology) and set conditions and price for accessing this information; (2) as consumer, according to the producer specific requirements, download and store information for its own usage; (3) as a relayer or augmentor, relay downloaded and/or post-processed information to another node including it's own conditions and price. 

All information uploaded to the network is hashed and connected to the producer, so the payments for that information can be redirected to the correct node.

For relayer or augmentor of a pre-existing information in the network, when a node request information for it, the requester would pay the producer for the information and the relayer/augmentor for the service, each of them according their settings.

A producer could upload certain information with specific conditions and negative price, which would mean that the producer is paying to make this information spread across the network, preserving it.

This should produce the correct incentives for the network participants to tend to keep the information with causal power, better ontology, structure and quality, slowly spurging or not even distributing simple data or bad information.

For all information produced, a discovery algorithm extract searchable metadata from the data ontology and structure and distribute it across the producers, relayers and augmentors making this metadata information discoverable from any active node of the network.

### Avoid information cloning
To address the problem of a bad actor downloading information from a information-producer and reuploading it in a new node as a bad-actor-new-producer: use the ontology and structure of the information to generate hashes associating the information with its producer, redirecting the payments for this information to the correct producer independently of who last uploaded that information into the network. The only way around this would be modifying the information so it generates new hashes, with would tamper with the information quality and accuracy making it naturally less valuable, less causal, less preserved onto the network.

### Information discoverability 
Every active node (producer, relayer and augmentor) must be available to receive discoverable metadata from others active node, and server this information via a searchable endpoint to eventual consumers.
> CHECK: might be a good idea to add a neutral network node for metadata/searchability/discoverability

